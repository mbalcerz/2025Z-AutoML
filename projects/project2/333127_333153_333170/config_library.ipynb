{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01351e6e",
   "metadata": {},
   "source": [
    "# Początkowa szeroka biblioteka modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10aa5ea",
   "metadata": {},
   "source": [
    "Biblioteka modeli została skonstruowana w oparciu o hierarchię skuteczności, stawiając zdecydowany nacisk na Gradient Boosting (XGBoost, CatBoost) oraz Lasy Losowe. Wybór ten podyktowany jest ich wysoką zdolnością generalizacji na surowych danych. Jednocześnie, w celu zapewnienia różnorodności predykcyjnej i stabilności ensamblu, pulę uzupełniono o modele o odmiennych biasach indukcyjnych (SVM, Regresja Logistyczna, Naive Bayes).\n",
    "\n",
    "Konfiguracja hiperparametrów nie opiera się na metodzie Brute Force, lecz na świadomym projektowaniu wariantów (Model Design Strategy):\n",
    "\n",
    "Balans Złożoności: Parametry mogące prowadzić do overfittingu (np. max_depth=10 w boostingach) są zawsze kontrowane parametrami regularyzacyjnymi (np. gamma, reg_lambda, min_samples_leaf), co zapewnia stabilność uczenia.\n",
    "\n",
    "Adaptacyjność: Zbiór konfiguracji zawiera zarówno \"płytkie\" estymatory dla prostych zadań, jak i rozbudowane struktury dla danych wielowymiarowych.\n",
    "\n",
    "Obsługa Nierównowagi: We wszystkich dostępnych algorytmach systemowo uwzględniono wagi klas (parametry typu class_weight lub strategie samplingu), minimalizując wpływ dysproporcji w danych treningowych na wynik końcowy.\n",
    "\n",
    "Informacje o paramerach i ich cechach pochodzą bezpośrednio z dokumentacji każdego z modeli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab893828",
   "metadata": {},
   "source": [
    "### Modele liniowe\n",
    "\n",
    "**Parametry - opis:**\n",
    "* **penalty** - określa rodzaj regularyzacji, czyli karę za zbyt duże współczynniki modelu.\n",
    "* **C** - odwrotność siły regularyzacji, określa jak bardzo pozwalamy modelowi ufać danym.\n",
    "* **solver** - algorytm optymalizacji, który znajduje najlepsze współczynniki.\n",
    "* **class_weight** - określa jak ważne są poszczególne klasy podczas uczenia modelu.\n",
    "* **max_iter** - maksymalna liczba kroków które solver może wykonać podczas uczenia, dobierany odpowiednio do pozostałych parametrów modelu.\n",
    "\n",
    "**Parametry ogólne:**\n",
    "* **penalty**: l1, **l2 (default)**, elasticnet, none\n",
    "* **C**: **1.0 (default)**\n",
    "* **solver**: saga, **lbfgs (default)**, liblinear, svd, lsqr\n",
    "* **class_weight**: balanced, **None (default)**\n",
    "* **max_iter**: **100 (default)**\n",
    "\n",
    "**Opis zastosowania:**\n",
    "\n",
    "---\n",
    "* **l1** -> zeruje nieistotne cechy; przydatne przy dużych, zaszumionych zbiorach, gdy podejrzewamy że tylko częśc cech jest istotna.\n",
    "* **l2** -> karze duże wagi, ale nie zeruje ich; wybieramy, gdy nie wiemy wiele o danych, gdy cechy wydaja sie w miarę sensowen, jest to stabliny wariant bazowy.\n",
    "* **l1 + l2 (elasticnet)** -> zeruje cechy, ale grupowo; dla danych skorelowanych, gdy wiele cech jest podobnych (np. cechy one-hot), jak chcemy uniknąć losowego wyboru jednej z wielu podobnych cech.\n",
    "---\n",
    "* **małe C (np. 0.01, 0.1)** -> silna regularyzacja; dobre gdy mało danych, dużo cech, dużo szumu, dane wysoko wymiarowe, ryzyko underfittingu.\n",
    "* **średnie C (np. 0.5, 1)** -> stabilne rozwiązanie; gdy nie wiemy nic o jakości cech.\n",
    "* **duże C (np. 10, 100)** -> słaba regularyzacja, model mocno dopasowuje się do danych; dobre dla bardzo dużych danych, niskiego poziomu szumu, ryzyko overfittingu.\n",
    "---\n",
    "* **saga** -> aktualizuje wagi na małych porcjach danych, obsługuje l1 i elasticnet; dobry dla dużych zbiorów danych, wysokiej liczby cech.\n",
    "* **lbfgs** -> algorytm quasi-newtonowski, klasyczny, stabilny wariant, obsługuje tylko l2; dobry jako domyślny solver gdy nie wiemy nic o danych.\n",
    "* **liblinear** -> oparty o optymalizację liniową; dobry dla małych zbiorów.\n",
    "* **LDA** -> solvery **lsqr** oraz **svd** są zalecane, gdy mamy więcej cech niż próbek.\n",
    "* **lsqr** -> gdy dane są skorelowane, **svd**: dla wysokowymiarowych, dużo cech.\n",
    "---\n",
    "* **balanced** -> próbki klasy mniejszościowej dostają większą wagę; dobre gdy nie znamy rozkładu klas, gdy mozliwa duża nierównowaga.\n",
    "\n",
    "---\n",
    "\n",
    "**Wybrane kombinacje (LogisticRegression):**\n",
    "\n",
    "1. **Standardowy model z solverem SAGA** (dobre dla dużych danych):\n",
    "   \"penalty\": \"l2\", \"C\": 1.0, \"solver\": \"saga\", \"class_weight\": \"balanced\", \"max_iter\": 2000\n",
    "\n",
    "2. **Klasyczny solver LBFGS** (stabilny):\n",
    "   \"penalty\": \"l2\", \"C\": 1.0, \"solver\": \"lbfgs\", \"class_weight\": \"balanced\", \"max_iter\": 2000\n",
    "\n",
    "3. **Regularyzacja L1** (selekcja cech):\n",
    "   \"penalty\": \"l1\", \"C\": 0.5, \"solver\": \"saga\", \"class_weight\": \"balanced\", \"max_iter\": 2000\n",
    "\n",
    "4. **ElasticNet** (dane skorelowane):\n",
    "   \"penalty\": \"elasticnet\", \"l1_ratio\": 0.5, \"C\": 0.1, \"solver\": \"saga\", \"class_weight\": \"balanced\", \"max_iter\": 3000\n",
    "\n",
    "5. **Silna regularyzacja dla mniejszych zbiorów** (małe, trudne dane):\n",
    "   \"penalty\": \"l2\", \"C\": 0.01, \"solver\": \"liblinear\", \"class_weight\": \"balanced\", \"max_iter\": 1000\n",
    "\n",
    "**LinearDiscriminantAnalysis (LDA):**\n",
    "* \"solver\": \"svd\" (dużo cech)\n",
    "* \"solver\": \"lsqr\", \"shrinkage\": \"auto\" (dane skorelowane)\n",
    "\n",
    "**RidgeClassifier:**\n",
    "* \"alpha\": 1.0, \"class_weight\": \"balanced\", \"solver\": \"auto\"\n",
    "\n",
    "---\n",
    "\n",
    "### Drzewa decyzyjne\n",
    "\n",
    "**Parametry - opis:**\n",
    "* **max_depth** - maksymalna głębokość drzewa, określa złożonośc modelu.\n",
    "* **min_samples_leaf** - minimalna liczba próbek w liściu, dobry parametr do walki z overfittingiem.\n",
    "* **min_samples_split** - minimalna liczba próbek do podziału, zabobiega dzieleniu bardzo małych grup.\n",
    "* **criterion** - określa miarę jakości podziału węzłów drzewa decyzyjnego (jak mierzymy dobry split).\n",
    "* **class_weight** - wagi klas.\n",
    "\n",
    "\n",
    "**Parametry bazowe:**\n",
    "* **max_depth**=None, **min_samples_leaf**=1, **min_samples_split**=2\n",
    "* **criterion**='gini'\n",
    "* **class_weight**=None\n",
    "\n",
    "\n",
    "**Opis zastosowania:**\n",
    "\n",
    "---\n",
    "* **max_depth (1-5)** -> proste relacje, dla małych danych i małego szumu.\n",
    "* **max_depth (15-20)** -> bardzo złożone relacje, gdy dużo danych.\n",
    "* **max_depth (None)** -> brak limitu, typowo do eksploracji.\n",
    "---\n",
    "* **min_samples_leaf(1)** -> maksymalna ekspresja, gdy dużo danych.\n",
    "* **min_samples_leaf(>10)**  -> mało danych, gdy jest szum.\n",
    "---\n",
    "* **gini** -> mierzy jak często losowo wybrana próbka byłaby źle sklasyfikowana, gdybyśmy losowo zgadywali klasę zgodnie z rozkładem w węźle; dobre dla dużych danych, gdy zależy na szybkości.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Konfiguracje:**\n",
    "\n",
    "1. **Bardzo płytkie (Decision Stump):**\n",
    "   \"max_depth\": 1, \"criterion\": \"gini\" -> jedna reguła decyzyjna; dla trywialnych danych.\n",
    "\n",
    "2. **Bezpieczny wybór (Średnia złożoność):**\n",
    "   \"max_depth\": 10, \"min_samples_leaf\": 5, \"class_weight\": \"balanced\"\n",
    "\n",
    "3. **Głębokie drzewo (Wysoka złożoność):**\n",
    "   \"max_depth\": 20, \"min_samples_split\": 10, \"class_weight\": \"balanced\", \"criterion\": \"gini\" -> potencjał dla złożonych granic, ryzyko overfittingu.\n",
    "\n",
    "4. **Bez ograniczeń (Eksploracja):**\n",
    "   \"max_depth\": null, \"min_samples_leaf\": 1, \"class_weight\": null -> ryzyko przeuczenia, do weryfikacji.\n",
    "\n",
    "---\n",
    "\n",
    "### Drzewa losowe (Random Forest)\n",
    "\n",
    "**Parametry bazowe:**\n",
    "* **n_estimators**=100, **criterion**='gini', **max_depth**=None\n",
    "* **min_samples_split**=2, **min_samples_leaf**=1\n",
    "\n",
    "**Konfiguracje:**\n",
    "\n",
    "1. **Szybki, prosty, płytki:**\n",
    "   \"n_estimators\": 100, \"max_depth\": 10, \"n_jobs\": 1, \"class_weight\": \"balanced\", \"random_state\": 42\n",
    "\n",
    "2. **Pełne drzewo (Mocny model):**\n",
    "   \"n_estimators\": 300, \"max_depth\": null, \"min_samples_leaf\": 2 -> brak ryzyka overfittingu (asymptotycznie).\n",
    "\n",
    "3. **Głębokie z ograniczeniami (Bezpieczniejsze):**\n",
    "   \"n_estimators\": 500, \"max_depth\": 15, \"min_samples_leaf\": 5, \"class_weight\": \"balanced\" -> większe min_samples_leaf patrzy bardziej na ogół.\n",
    "\n",
    "4. **Eksploracja entropii:**\n",
    "   \"n_estimators\": 200, \"criterion\": \"entropy\", \"max_depth\": 25, \"min_samples_split\": 5 -> inny rodzaj kryterium dla różnorodności.\n",
    "\n",
    "**ExtraTreesClassifier:**\n",
    "1. **Wariant zrównoważony:** \"n_estimators\": 200, \"max_depth\": 12, \"min_samples_leaf\": 2, \"bootstrap\": true, \"class_weight\": \"balanced\"\n",
    "2. **Wariant głęboki:** \"n_estimators\": 400, \"max_depth\": null, \"criterion\": \"entropy\"\n",
    "\n",
    "---\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "**Parametry - opis:**\n",
    "* **learning_rate** - krok uczenia, określa jak bardzo każde kolejne drzewo wpływa na wynik.\n",
    "* **n_estimators** - liczba drzew w esemble.\n",
    "* **max_depth** - maksymalna głębokość pojedyńczego drzewa.\n",
    "* **subsample** - procent próbek używanych do budowy każdego drzewa.\n",
    "* **colsample_bytree** - procent cech używanych do budowy drzewa.\n",
    "* **reg_lambda** - regularyzacja wag liści L2, zmniejsza wagi liści.\n",
    "* **reg_alpha** - regularyzacja wag liści L1, zeruje nieistotne liście.\n",
    "* **gamma** - próg opłacalności podziału w drzewie, dobry do walki z overfittingiem.\n",
    "* **min_child_weight** - minimalna suma wag próbek w liściu, chroni przed liścmi opartymi o pojedyńcze próbki.\n",
    "\n",
    "**Parametry (defaults):**\n",
    "* **learning_rate** (0.3), **n_estimators** (rounds), **max_depth** (6)\n",
    "* **subsample** (1), **colsample_bytree** (1)\n",
    "* **reg_lambda** (1), **reg_alpha** (0)\n",
    "* **gamma** (0)\n",
    "* **min_child_weight** (1)\n",
    "\n",
    "**Opis zastosowania:**\n",
    "\n",
    "---\n",
    "* **learning_rate (0.01-0.05)** -> bardzo wolne, stabilne uczenie; dobre dla dużych i czystych danych.\n",
    "* **learning_rate (0.1-0.3)** -> szybsze i bardzoiej agresywne; lepsze dla małych zbiorów.\n",
    "---\n",
    "* **subsample (0.7-0.9)** -> umiarkowana regularyzacja, gdy pojawia się overfitting, dobrze jest zmniejszać; dla wiekszych danych mniejsze.\n",
    "* **colsample_bytree (0.6-0.8)** -> standard, kluczowy przy danych wysokowymairowych.\n",
    "* **colsample_bytree (0.4-0.6)** -> silna redukcja korelacji; gdy jest bardzo dużo cech.\n",
    "---\n",
    "* **gamma (0)** -> każdy sensowny split; dla dużych danych.\n",
    "* **gamma (0.1-.05)** -> odzruca słabe splity.\n",
    "\n",
    "---\n",
    "\n",
    "**Konfiguracje:**\n",
    "\n",
    "1. **Płytki, szybko uczący się:**\n",
    "   \"n_estimators\": 100, \"learning_rate\": 0.1, \"max_depth\": 4, \"subsample\": 0.8\n",
    "\n",
    "2. **Głębszy, umiarkowana szybkość:**\n",
    "   \"n_estimators\": 400, \"learning_rate\": 0.05, \"max_depth\": 7, \"subsample\": 0.9, \"colsample_bytree\": 0.8\n",
    "\n",
    "3. **Średni model (Baseline):**\n",
    "   \"n_estimators\": 150, \"max_depth\": 6, \"learning_rate\": 0.1\n",
    "\n",
    "4. **Slow Learner (Bardzo wolna nauka):**\n",
    "   \"n_estimators\": 1000, \"learning_rate\": 0.01, \"max_depth\": 6, \"subsample\": 0.7, \"colsample_bytree\": 0.7\n",
    "\n",
    "5. **Głęboki z ograniczeniami:**\n",
    "   \"n_estimators\": 200, \"learning_rate\": 0.05, \"max_depth\": 10, \"min_child_weight\": 5, \"gamma\": 0.2\n",
    "\n",
    "6. **Regularyzacja (L1/L2):**\n",
    "   \"n_estimators\": 300, \"learning_rate\": 0.05, \"max_depth\": 5, \"reg_alpha\": 0.5, \"reg_lambda\": 1.0\n",
    "\n",
    "---\n",
    "\n",
    "### CatBoostClassifier\n",
    "\n",
    "**Parametry - opis:**\n",
    "* **iterations** - liczba drzew dodanych do ensemble (odpowiednik n_estimators), więcej iteracji = większa zdolność dopasowania, ale wolniejsze uczenie.\n",
    "* **depth** * - głębokość symetrycznego dzrewa.\n",
    "* **l2_leaf_reg** - kara za duże wagi, zapobiega przeuczeniu.\n",
    "* **bagging_temperature** - wagi obserwacji z rozkładu wykładniczego (Bayesian Bootstrap), kontroluje jak losowo wybierane są próbki, 0 -> brak losowości.\n",
    "* **random_strength** - losowość przy wyborze podziału.\n",
    "\n",
    "**Parametry (defaults):**\n",
    "* **iterations** (1000), **depth** (6), **l2_leaf_reg** (3)\n",
    "* **bagging_temperature** (1), **random_strength** (1), **learning_rate** (0.3)\n",
    "\n",
    "**Opis zastosowania:**\n",
    "\n",
    "---\n",
    "* **bagging_temperature (0.5-1)** -> umairkowana losowość, im większa wartośc tym większa losowość; większa dobra dla dużych danych.\n",
    "---\n",
    "\n",
    "**Konfiguracje:**\n",
    "\n",
    "1. **Standardowy:**\n",
    "   \"iterations\": 150, \"depth\": 6, \"learning_rate\": 0.15\n",
    "\n",
    "2. **Wolniejszy z regularyzacją:**\n",
    "   \"iterations\": 500, \"depth\": 8, \"learning_rate\": 0.05, \"l2_leaf_reg\": 3\n",
    "\n",
    "3. **Mocny model (Intensive):**\n",
    "   \"iterations\": 1000, \"learning_rate\": 0.03, \"depth\": 8, \"l2_leaf_reg\": 5, \"bagging_temperature\": 1\n",
    "\n",
    "4. **Długodystansowiec:**\n",
    "   \"iterations\": 2000, \"learning_rate\": 0.02, \"depth\": 8, \"l2_leaf_reg\": 5, \"bagging_temperature\": 1\n",
    "\n",
    "5. **Łagodniejszy wariant:**\n",
    "   \"iterations\": 800, \"learning_rate\": 0.04, \"depth\": 8, \"l2_leaf_reg\": 5, \"bagging_temperature\": 1\n",
    "\n",
    "6. **Płytki (Small):**\n",
    "   \"iterations\": 600, \"depth\": 4, \"learning_rate\": 0.1, \"l2_leaf_reg\": 3\n",
    "\n",
    "7. **Deep Slow (Eksperymentalny):**\n",
    "   \"iterations\": 1500, \"depth\": 10, \"learning_rate\": 0.01, \"l2_leaf_reg\": 9, \"random_strength\": 1\n",
    "\n",
    "---\n",
    "\n",
    "### KNN (K-Nearest Neighbors)\n",
    "\n",
    "**Parametry - opis:**\n",
    "* **n_neighbours** - liczba najbliższych obserwacji ze zbioru treningowego, które są brane pod uwagę przy wyznaczaniu klasy.\n",
    "* **weights** - określa jak liczy się głosy sąsiadów.\n",
    "* **p** -definiuje jak liczona jest odległość.\n",
    "\n",
    "**Parametry (defaults):**\n",
    "* **n_neighbours** (5), **weights** (\"uniform\"), \"distance\" \n",
    "* **p** (2)\n",
    "\n",
    "**Opis zastosowania:**\n",
    "\n",
    "---\n",
    "* **uniform** -> każdy sąsiad ma taki sam wpływ, niezależnie od odległości (uśrednia).\n",
    "* **distance** -> głos jest ważony odwrotnie proporcjonalnie do odległości (słucha najbliższych); lepsze przy nierównej gęstości danych, odporne na szum.\n",
    "---\n",
    "* **p=1 (Manhattan)** -> odporne na wartości odstające, większa odporność na szum.\n",
    "* **p=2** -> standard dla danych ciągłych, wrażliwa na skalę.\n",
    "\n",
    "\n",
    "**Konfiguracje:**\n",
    "1. **Średnie K, wagi odległościowe:** \"n_neighbors\": 15, \"weights\": \"distance\"\n",
    "2. **Duże K:** \"n_neighbors\": 25, \"weights\": \"distance\"\n",
    "3. **Domyślny:** \"n_neighbors\": 5, \"weights\": \"uniform\"\n",
    "4. **Wysokowymiarowy:** \"n_neighbors\": 50, \"weights\": \"distance\", \"p\": 1 \n",
    "\n",
    "---\n",
    "\n",
    "### SVC (Support Vector Classifier)\n",
    "\n",
    "**Parametry - opis:**\n",
    "* **C** - parametr regularizacji, kontroluje karę za błędną klasyfikację próbek treningowych.\n",
    "* **kernel** - określa postać transformacji przestrzeni cech, w której budowana jest hiperpłaszczyzna separująca.\n",
    "* **gamma** - określa zasięg wpływu pojedynczej próbki treningowej w przestrzeni jądra.\n",
    "* **probability** - Określa, czy model ma zwracać prawdopodobieństwa klas zamiast samych etykiet.\n",
    "* **class_weight** - wagi przypisane klasom, używane do modyfikacji funkcji straty w przypadku niezbalansowanych danych.\n",
    "* **cache_size** - rozmiar pamięci podręcznej (w MB) przeznaczonej na przechowywanie obliczeń macierzy jądra w trakcie uczenia.\n",
    "\n",
    "**Parametry (defaults):**\n",
    "* **C** (default 1.0), **kernel** (rbf), linear, poly\n",
    "* **gamma** (scale), **probability** (False), True\n",
    "* **class_weight** (None), \"balanced\", **cache_size** (200)\n",
    "\n",
    "**Opis zastosowania:**\n",
    "\n",
    "---\n",
    "* **rbf** -> mapuje dane do nieskończenie wymiarowej przestrzeni, umożliwiając nieliniową separację; do danych nieliniowych, gdy nie znamy struktury danych\n",
    "* **linear** -> buduje liniową hiperpłaszczyznę; dla danytch liniowo separowalnych, dużo cech, mało próbek, gdy dane prawie liniowe\n",
    "* **poly** -> jądro wielomianowe; do niskowymiarowych danych\n",
    "---\n",
    "* **probabilty (True)** -> wymagane do AUC.\n",
    "---\n",
    "\n",
    "\n",
    "**Konfiguracje:**\n",
    "1. **Bazowy o średnich parametrach**\n",
    "    \"max_depth\": 7, \"class_weight\": \"balanced\",\"criterion\": \"entropy\"\n",
    "\n",
    "1. **Standardowy RBF:**\n",
    "   \"C\": 1.0, \"kernel\": \"rbf\", \"gamma\": \"scale\", \"probability\": true, \"class_weight\": \"balanced\", \"cache_size\": 1000\n",
    "\n",
    "2. **Liniowy (Soft Margin):**\n",
    "   \"C\": 0.1, \"kernel\": \"linear\", \"probability\": true, \"class_weight\": \"balanced\"\n",
    "\n",
    "3. **Wielomianowy:**\n",
    "   \"C\": 1.0, \"kernel\": \"poly\", \"degree\": 3, \"probability\": true, \"class_weight\": \"balanced\"\n",
    "\n",
    "4. **Hard Margin (Rygorystyczny):**\n",
    "   \"C\": 10.0, \"kernel\": \"rbf\", \"gamma\": \"scale\", \"probability\": true\n",
    "\n",
    "---\n",
    "\n",
    "### Inne modele\n",
    "\n",
    "* **sklearn.naive_bayes.GaussianNB**\n",
    "* **sklearn.naive_bayes.BernoulliNB**: {\"alpha\": 0.5} oraz {\"alpha\": 1.0} (test wpływu regularyzacji na dane binarne)\n",
    "* **sklearn.ensemble.AdaBoostClassifier**: {\"n_estimators\": 100, \"learning_rate\": 1.0} oraz {\"n_estimators\": 200, \"learning_rate\": 0.1} (szybkie i agresywne uczenie vs mniejsze kroki i stabilniej)\n",
    "* **sklearn.ensemble.GradientBoostingClassifier**: {\"n_estimators\": 200, \"learning_rate\": 0.1, \"max_depth\": 3, \"subsample\": 0.8, \"validation_fraction\": 0.1, \"n_iter_no_change\": 10} (klasyczne, stabline)\n",
    "* **sklearn.ensemble.HistGradientBoostingClassifier**: {\"max_iter\": 200, \"learning_rate\": 0.1, \"max_depth\": 10, \"l2_regularization\": 0.5} (szybki, skalowany na wieksze dane)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334bf67",
   "metadata": {},
   "source": [
    "W celu zapewnienia pełnej powtarzalności eksperymentu, parametry random_state zastosowano we wszystkich algorytmach niedeterministycznych. Modele takie jak LDA, KNN oraz naiwne klasyfikatory Bayesa (GaussianNB, BernoulliNB) działają w sposób deterministyczny, dlatego nie wymagają tego ustawienia.\n",
    "\n",
    "W przypadku Lasów Losowych zdecydowano się na pracę jednowątkową (n_jobs=1), co eliminuje ryzyko potencjalnych różnic numerycznych wynikających z wielowątkowości, kosztem dłuższego czasu trenowania.\n",
    "\n",
    "Dla algorytmu CatBoost skonfigurowano parametry techniczne: wyłączono zapis plików na dysku (allow_writing_files=False) oraz wyciszono standardowe wyjście modelu (verbose=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e20ad",
   "metadata": {},
   "source": [
    "Powyższe konfiguracja zostały zapisane w pliku models_for_test.json do dalszej selekcji."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
